{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import utils\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import *\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siema!\n",
    "Warsztat KNUM 05.04.2019.\n",
    "\n",
    "Do zrobienia:\n",
    "1. Zaimplementuj early stopping. Trzymaj licznik, od ilu epok loss nie spada, jeśli przekroczy daną liczbę epok, to kończ trening.\n",
    "2. Zmniejsz lub zwiększ learning rate. Czy loss nadal spada? Szybciej czy wolniej?\n",
    "3. Przetestuj inne warotści batch size.\n",
    "4. Wypróbuj głębsze modele (np 3, 4 albo 8 warstw). Zobacz, czy zachodzi overfitting.\n",
    "5. Dodaj regularuzację L2. Dodaje się ją do optimizera za pomocą argumentu `weight_decay` w konstruktorze. Czy nadal zachodzi overfitting? Czy sieć uczy się tak samo szybko?\n",
    "6. Powiększ jeszcze sieć. Jakie są wyniki? Do powiększonej sieci dodaj warstwę dropout, wypróbuj go z różnymi wartościami prawdopodobieńśtwa wyzerowania wagi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineRegressor(nn.Module):\n",
    "    '''\n",
    "    Baseline Reggressor architecture\n",
    "    input [13] -> [13] -> [1] output\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BaselineRegressor, self).__init__()\n",
    "        self.layer1 = nn.Linear(13, 13)\n",
    "        self.layer2 = nn.Linear(13, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    batch_size=32,\n",
    "    learning_rate=3e-3,\n",
    "    n_epochs=5,\n",
    "):\n",
    "    '''\n",
    "    This function trains the model on boston housing dataset.\n",
    "    At each epoch it trains model on train set and then evaluates if on test set.\n",
    "    Function implements early stopping, when model overfits training shutdowns.\n",
    "\n",
    "    :param model: Model instance to be trained\n",
    "    :param batch_size: Size of batch\n",
    "    :param learning_rate: Learning rate for optimizer (Adam)\n",
    "    :param n_epochs: Number of training epochs\n",
    "    :return: (best model, loss of best model)\n",
    "    '''\n",
    "    train_set, val_set = load_data()\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=learning_rate,\n",
    "                           )\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # train loop\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            model.zero_grad()\n",
    "            y = y.view(-1, 1)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # evaluation loop\n",
    "        loss_sum = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                y = y.view(-1, 1)\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "                loss_sum += loss.item()\n",
    "\n",
    "        avg_loss = loss_sum / (len(val_set) / batch_size)\n",
    "        print(f\"Epoch: {epoch} loss: {avg_loss}\")\n",
    "\n",
    "\n",
    "    return model, avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 73.00936924190972\n",
      "Epoch: 1 loss: 48.95055222699023\n",
      "Epoch: 2 loss: 46.227620229946346\n",
      "Epoch: 3 loss: 41.933897743074915\n",
      "Epoch: 4 loss: 47.38390167867105\n",
      "Epoch: 5 loss: 41.55116002390704\n",
      "Epoch: 6 loss: 42.207674548381895\n",
      "Epoch: 7 loss: 40.1064343339815\n",
      "Epoch: 8 loss: 51.82494223587157\n",
      "Epoch: 9 loss: 45.75264461397186\n",
      "Epoch: 10 loss: 45.000009649381866\n",
      "Epoch: 11 loss: 40.2383227235689\n",
      "Epoch: 12 loss: 37.05767787347629\n",
      "Epoch: 13 loss: 38.56088262843335\n",
      "Epoch: 14 loss: 38.417118853471415\n",
      "Epoch: 15 loss: 40.52827395792083\n",
      "Epoch: 16 loss: 35.22653767443079\n",
      "Epoch: 17 loss: 31.567960761663485\n",
      "Epoch: 18 loss: 32.40372684058242\n",
      "Epoch: 19 loss: 40.41415800259808\n",
      "Epoch: 20 loss: 37.23214757724071\n",
      "Epoch: 21 loss: 35.210369215236874\n",
      "Epoch: 22 loss: 31.010203061141368\n",
      "Epoch: 23 loss: 27.352968764117385\n",
      "Epoch: 24 loss: 26.818615451572448\n",
      "Epoch: 25 loss: 25.84191115822379\n",
      "Epoch: 26 loss: 35.94636909605011\n",
      "Epoch: 27 loss: 31.879872487285947\n",
      "Epoch: 28 loss: 25.71767112401527\n",
      "Epoch: 29 loss: 25.322231044919473\n",
      "Epoch: 30 loss: 32.92507937574011\n",
      "Epoch: 31 loss: 23.929677891918995\n",
      "Epoch: 32 loss: 23.677416035509488\n",
      "Epoch: 33 loss: 26.63449621576024\n",
      "Epoch: 34 loss: 23.072924050759145\n",
      "Epoch: 35 loss: 26.541870316182536\n",
      "Epoch: 36 loss: 23.797592290743133\n",
      "Epoch: 37 loss: 22.857130484318173\n",
      "Epoch: 38 loss: 23.729062690509586\n",
      "Epoch: 39 loss: 22.81000480877133\n",
      "Epoch: 40 loss: 23.673649115825263\n",
      "Epoch: 41 loss: 23.43326694383396\n",
      "Epoch: 42 loss: 22.463523797162875\n",
      "Epoch: 43 loss: 26.702239569716568\n",
      "Epoch: 44 loss: 24.74435982741709\n",
      "Epoch: 45 loss: 25.614462630955256\n",
      "Epoch: 46 loss: 22.634937556709833\n",
      "Epoch: 47 loss: 28.02581458579837\n",
      "Epoch: 48 loss: 29.03449261282373\n",
      "Epoch: 49 loss: 26.04466763068372\n",
      "BaselineRegressor(\n",
      "  (layer1): Linear(in_features=13, out_features=13, bias=True)\n",
      "  (layer2): Linear(in_features=13, out_features=1, bias=True)\n",
      ") 26.04466763068372\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "model = BaselineRegressor()\n",
    "model, loss = train(\n",
    "    model,\n",
    "    batch_size=5,\n",
    "    learning_rate=1e-3,\n",
    "    n_epochs=50\n",
    ")\n",
    "\n",
    "print(model, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
