{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import utils\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import *\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siema!\n",
    "Warsztat KNUM 05.04.2019, autorstwa Mateusza Olko i Julii Bazińskiej. \n",
    "\n",
    "Do zrobienia:\n",
    "1. Zaimplementuj early stopping. Trzymaj licznik, od ilu epok loss nie spada, jeśli przekroczy daną liczbę epok, to kończ trening.\n",
    "2. Zmniejsz lub zwiększ learning rate. Czy loss nadal spada? Szybciej czy wolniej?\n",
    "3. Przetestuj inne warotści batch size.\n",
    "4. Wypróbuj głębsze modele (np 3, 4 albo 8 warstw). Zobacz, czy zachodzi overfitting.\n",
    "5. Dodaj regularyzację L2. Dodaje się ją do optimizera za pomocą argumentu `weight_decay` w konstruktorze. Czy nadal zachodzi overfitting? Czy sieć uczy się tak samo szybko?\n",
    "6. Powiększ jeszcze sieć. Jakie są wyniki? Do powiększonej sieci dodaj warstwę dropout, wypróbuj go z różnymi wartościami prawdopodobieńśtwa wyzerowania wagi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineRegressor(nn.Module):\n",
    "    '''\n",
    "    Baseline Reggressor architecture\n",
    "    input [13] -> [13] -> [1] output\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BaselineRegressor, self).__init__()\n",
    "        self.layer1 = nn.Linear(13, 13)\n",
    "        self.layer2 = nn.Linear(13, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DeeperRegressor(nn.Module):\n",
    "    '''\n",
    "    Deeper Reggressor architecture\n",
    "    input [13] -> [13] -> [6] -> [1] output\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DeeperRegressor, self).__init__()\n",
    "        self.layer1 = nn.Linear(13, 13)\n",
    "        self.layer2 = nn.Linear(13, 6)\n",
    "        self.layer3 = nn.Linear(6, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class DropoutRegressor(nn.Module):\n",
    "    '''\n",
    "    Dropout Reggressor architecture\n",
    "    This net is much bigger then the others to make dropout useful.\n",
    "    You usually put dropout before last layer. \n",
    "    input [13] -> [30] -> [40] -> Dropout -> [1] output\n",
    "    '''\n",
    "\n",
    "    def __init__(self, dropout_prob=0.1):\n",
    "        '''\n",
    "        :param dropout_prob: Probability that activation of a neuron\n",
    "                             will be dropped\n",
    "        '''\n",
    "        super(DropoutRegressor, self).__init__()\n",
    "        self.layer1 = nn.Linear(13, 30)\n",
    "        self.layer2 = nn.Linear(30, 40)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.last = nn.Linear(40, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.last(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    batch_size=32,\n",
    "    learning_rate=3e-3,\n",
    "    n_epochs=5,\n",
    "    max_overfit=3,\n",
    "    l2_reg=0.0001\n",
    "):\n",
    "    '''\n",
    "    This function trains the model on boston housing dataset.\n",
    "    At each epoch it trains model on train set and then evaluates if on test set.\n",
    "    Function implements early stopping, when model overfits training shutdowns.\n",
    "\n",
    "    :param model: Model instance to be trained\n",
    "    :param batch_size: Size of batch\n",
    "    :param learning_rate: Learning rate for optimizer (Adam)\n",
    "    :param n_epochs: Number of training epochs\n",
    "    :param max_overfit: Max number of epochs of loss not improving before stopping training.\n",
    "    :return: (best model, loss of best model)\n",
    "    '''\n",
    "    train_set, val_set = load_data()\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=learning_rate,\n",
    "                           weight_decay=l2_reg,\n",
    "                           )\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    overfit_epochs = 0\n",
    "    best_model_loss = 1e10\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # train loop\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            model.zero_grad()\n",
    "            y = y.view(-1, 1)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # evaluation loop\n",
    "        loss_sum = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                y = y.view(-1, 1)\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "                loss_sum += loss.item()\n",
    "\n",
    "        avg_loss = loss_sum / (len(val_set) / batch_size)\n",
    "        print(f\"Epoch: {epoch} loss: {avg_loss}\")\n",
    "\n",
    "        # Early stopping.\n",
    "        if best_model_loss > loss_sum:\n",
    "            best_model_loss = loss_sum\n",
    "            best_model = model\n",
    "            overfit_epochs = 0\n",
    "        else:\n",
    "            overfit_epochs += 1\n",
    "\n",
    "        if overfit_epochs > max_overfit:\n",
    "            break\n",
    "            \n",
    "    return model, avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 715.7615961990957\n",
      "Epoch: 1 loss: 248.62603735736036\n",
      "Epoch: 2 loss: 94.42329238249562\n",
      "Epoch: 3 loss: 65.02235133816878\n",
      "Epoch: 4 loss: 49.35225559970526\n",
      "Epoch: 5 loss: 50.858955646124414\n",
      "Epoch: 6 loss: 46.91896990528257\n",
      "Epoch: 7 loss: 44.87835257072149\n",
      "Epoch: 8 loss: 44.570107234744576\n",
      "Epoch: 9 loss: 41.106691904894014\n",
      "Epoch: 10 loss: 42.416149086839575\n",
      "Epoch: 11 loss: 40.95740541698426\n",
      "Epoch: 12 loss: 41.89474813581452\n",
      "Epoch: 13 loss: 39.99258488182008\n",
      "Epoch: 14 loss: 47.58584262817864\n",
      "Epoch: 15 loss: 36.096802902972605\n",
      "Epoch: 16 loss: 34.515557739678336\n",
      "Epoch: 17 loss: 34.43123363134429\n",
      "Epoch: 18 loss: 34.27638534485825\n",
      "Epoch: 19 loss: 32.39877250250869\n",
      "Epoch: 20 loss: 33.81286440871832\n",
      "Epoch: 21 loss: 29.67312345354576\n",
      "Epoch: 22 loss: 27.829016963327966\n",
      "Epoch: 23 loss: 26.777523164674054\n",
      "Epoch: 24 loss: 33.405438746054344\n",
      "Epoch: 25 loss: 29.198411374580203\n",
      "Epoch: 26 loss: 26.882384142537756\n",
      "Epoch: 27 loss: 25.44167337455149\n",
      "Epoch: 28 loss: 28.1333375164843\n",
      "Epoch: 29 loss: 23.331839564278372\n",
      "Epoch: 30 loss: 22.674830701407487\n",
      "Epoch: 31 loss: 22.72727227586461\n",
      "Epoch: 32 loss: 22.35660690022266\n",
      "Epoch: 33 loss: 24.516488961347445\n",
      "Epoch: 34 loss: 21.455752328624875\n",
      "Epoch: 35 loss: 21.348012338473104\n",
      "Epoch: 36 loss: 21.696234185864608\n",
      "Epoch: 37 loss: 21.01337374664667\n",
      "Epoch: 38 loss: 20.586505358613383\n",
      "Epoch: 39 loss: 21.368443543516747\n",
      "Epoch: 40 loss: 31.80608794445128\n",
      "Epoch: 41 loss: 21.513216870976247\n",
      "Epoch: 42 loss: 20.089350600880902\n",
      "Epoch: 43 loss: 21.38698023135268\n",
      "Epoch: 44 loss: 25.02009943714292\n",
      "Epoch: 45 loss: 27.879559974970782\n",
      "Epoch: 46 loss: 19.93212200525239\n",
      "Epoch: 47 loss: 24.418460294956297\n",
      "Epoch: 48 loss: 19.301405659341437\n",
      "Epoch: 49 loss: 19.659964446946393\n",
      "Epoch: 50 loss: 22.953225458700828\n",
      "Epoch: 51 loss: 21.044323406820222\n",
      "Epoch: 52 loss: 18.884011234824115\n",
      "Epoch: 53 loss: 20.55028679333334\n",
      "Epoch: 54 loss: 20.985127584202083\n",
      "Epoch: 55 loss: 19.339167081464932\n",
      "Epoch: 56 loss: 19.026435435287596\n",
      "Epoch: 57 loss: 19.13786035823071\n",
      "Epoch: 58 loss: 18.817607183156053\n",
      "Epoch: 59 loss: 19.02180281211072\n",
      "Epoch: 60 loss: 19.583106012794918\n",
      "Epoch: 61 loss: 18.863232996989424\n",
      "Epoch: 62 loss: 18.73667974171676\n",
      "Epoch: 63 loss: 20.299541809427456\n",
      "Epoch: 64 loss: 18.28955484656837\n",
      "Epoch: 65 loss: 19.13081181330944\n",
      "Epoch: 66 loss: 18.308991198464643\n",
      "Epoch: 67 loss: 19.090776279216676\n",
      "Epoch: 68 loss: 19.204338704507183\n",
      "Epoch: 69 loss: 19.413043363826482\n",
      "Epoch: 70 loss: 19.873464651933805\n",
      "Epoch: 71 loss: 23.331753250182146\n",
      "Epoch: 72 loss: 21.23850938842053\n",
      "Epoch: 73 loss: 21.567513060382034\n",
      "Epoch: 74 loss: 19.440953046318114\n",
      "Epoch: 75 loss: 19.045102126955047\n",
      "DeeperRegressor(\n",
      "  (layer1): Linear(in_features=13, out_features=13, bias=True)\n",
      "  (layer2): Linear(in_features=13, out_features=6, bias=True)\n",
      "  (layer3): Linear(in_features=6, out_features=1, bias=True)\n",
      ") 19.045102126955047\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "model = DeeperRegressor()\n",
    "model, loss = train(\n",
    "    model,\n",
    "    batch_size=5,\n",
    "    learning_rate=1e-3,\n",
    "    n_epochs=100,\n",
    "    max_overfit=10\n",
    ")\n",
    "\n",
    "print(model, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 52.41513259767547\n",
      "Epoch: 1 loss: 47.20830421748124\n",
      "Epoch: 2 loss: 44.47108831931287\n",
      "Epoch: 3 loss: 50.8652522995716\n",
      "Epoch: 4 loss: 40.03758842550864\n",
      "Epoch: 5 loss: 39.19388864922711\n",
      "Epoch: 6 loss: 39.12170406401627\n",
      "Epoch: 7 loss: 35.86967529274347\n",
      "Epoch: 8 loss: 35.134203077301265\n",
      "Epoch: 9 loss: 42.69626974120854\n",
      "Epoch: 10 loss: 36.5405582442997\n",
      "Epoch: 11 loss: 49.00527691277932\n",
      "Epoch: 12 loss: 31.12774113031823\n",
      "Epoch: 13 loss: 46.36176950349583\n",
      "Epoch: 14 loss: 39.26652367659441\n",
      "Epoch: 15 loss: 22.821628315242258\n",
      "Epoch: 16 loss: 22.823993436933502\n",
      "Epoch: 17 loss: 21.893306487188564\n",
      "Epoch: 18 loss: 22.800854660394624\n",
      "Epoch: 19 loss: 21.66941151844235\n",
      "Epoch: 20 loss: 21.6598980774091\n",
      "Epoch: 21 loss: 18.490000564282337\n",
      "Epoch: 22 loss: 23.767613125598338\n",
      "Epoch: 23 loss: 18.31912462636242\n",
      "Epoch: 24 loss: 18.659377954606935\n",
      "Epoch: 25 loss: 19.348938864047135\n",
      "Epoch: 26 loss: 19.20742992341049\n",
      "Epoch: 27 loss: 24.18692925783593\n",
      "Epoch: 28 loss: 18.33774029269932\n",
      "Epoch: 29 loss: 20.21329366316007\n",
      "Epoch: 30 loss: 23.050112423934337\n",
      "Epoch: 31 loss: 18.330324747430996\n",
      "Epoch: 32 loss: 19.35847271145798\n",
      "Epoch: 33 loss: 18.199350974691196\n",
      "Epoch: 34 loss: 20.319910856682487\n",
      "Epoch: 35 loss: 15.939846902381717\n",
      "Epoch: 36 loss: 15.755074737578866\n",
      "Epoch: 37 loss: 16.260841390279335\n",
      "Epoch: 38 loss: 16.23023117621114\n",
      "Epoch: 39 loss: 16.8199159686021\n",
      "Epoch: 40 loss: 15.950202449100225\n",
      "Epoch: 41 loss: 16.076100496795235\n",
      "Epoch: 42 loss: 17.10865148174481\n",
      "Epoch: 43 loss: 16.96506466452531\n",
      "Epoch: 44 loss: 17.253670645511058\n",
      "Epoch: 45 loss: 17.50739458039051\n",
      "Epoch: 46 loss: 16.90474055414125\n",
      "Epoch: 47 loss: 17.194250256527127\n",
      "DropoutRegressor(\n",
      "  (layer1): Linear(in_features=13, out_features=30, bias=True)\n",
      "  (layer2): Linear(in_features=30, out_features=40, bias=True)\n",
      "  (dropout): Dropout(p=0.1)\n",
      "  (last): Linear(in_features=40, out_features=1, bias=True)\n",
      ") 17.194250256527127\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "model = DropoutRegressor()\n",
    "model, loss = train(\n",
    "    model,\n",
    "    batch_size=5,\n",
    "    learning_rate=1e-3,\n",
    "    n_epochs=100,\n",
    "    max_overfit=10\n",
    ")\n",
    "\n",
    "print(model, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
